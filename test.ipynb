{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "/Users/lvhaoran/opt/anaconda3/envs/mxnet/lib/python3.6/site-packages/gluonts/json.py:46: UserWarning: Using `json`-module for json-handling. Consider installing one of `orjson`, `ujson` to speed up serialization and deserialization.\n",
      "  \"Using `json`-module for json-handling. \"\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from gluonts.dataset.rolling_dataset import StepStrategy, generate_rolling_dataset\n",
    "import argparse\n",
    "import os\n",
    "import logging\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from gluonts.dataset.common import ListDataset\n",
    "from gluonts.dataset.field_names import FieldName\n",
    "from gluonts.dataset.util import to_pandas\n",
    "from gluonts.dataset.multivariate_grouper import MultivariateGrouper\n",
    "\n",
    "from gluonts.model.canonical import CanonicalRNNEstimator\n",
    "from gluonts.model.deep_factor import DeepFactorEstimator\n",
    "from gluonts.model.deepar import DeepAREstimator\n",
    "from gluonts.model.deepstate import DeepStateEstimator\n",
    "from gluonts.model.deepvar import DeepVAREstimator\n",
    "from gluonts.model.gp_forecaster import GaussianProcessEstimator\n",
    "from gluonts.model.gpvar import GPVAREstimator\n",
    "from gluonts.model.lstnet import LSTNetEstimator\n",
    "from gluonts.model.n_beats import NBEATSEstimator\n",
    "from gluonts.model.seq2seq import MQCNNEstimator, MQRNNEstimator, RNN2QRForecaster, Seq2SeqEstimator\n",
    "from gluonts.model.simple_feedforward import SimpleFeedForwardEstimator\n",
    "from gluonts.model.transformer import TransformerEstimator\n",
    "from gluonts.model.wavenet import WaveNetEstimator\n",
    "\n",
    "from gluonts.mx.block.quantile_output import QuantileOutput\n",
    "from gluonts.mx.trainer import Trainer\n",
    "from gluonts.mx.block.encoder import Seq2SeqEncoder\n",
    "\n",
    "from gluonts.model.predictor import Predictor\n",
    "\n",
    "from gluonts.model.naive_2 import Naive2Predictor\n",
    "from gluonts.model.npts import NPTSPredictor\n",
    "from gluonts.model.prophet import ProphetPredictor\n",
    "from gluonts.model.r_forecast import RForecastPredictor\n",
    "from gluonts.model.seasonal_naive import SeasonalNaivePredictor\n",
    "\n",
    "from gluonts.evaluation.backtest import make_evaluation_predictions\n",
    "from gluonts.evaluation import Evaluator\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import metrics\n",
    "def mape(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_pred - y_true) / y_true)) * 100\n",
    "\n",
    "def smape(y_true, y_pred):\n",
    "    return 2.0 * np.mean(np.abs(y_pred - y_true) / (np.abs(y_pred) + np.abs(y_true))) * 100\n",
    "\n",
    "def load_json(filename):\n",
    "    data = []\n",
    "    with open(filename, 'r') as fin:\n",
    "        while True:\n",
    "            line = fin.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            datai = json.loads(line)\n",
    "            data.append(datai)\n",
    "    return data\n",
    "def parse_data(dataset):\n",
    "    data = []\n",
    "    for t in dataset:\n",
    "        datai = {FieldName.TARGET: t['target'], FieldName.START: t['start']}\n",
    "        if 'id' in t:\n",
    "            datai[FieldName.ITEM_ID] = t['id']\n",
    "        if 'cat' in t:\n",
    "            datai[FieldName.FEAT_STATIC_CAT] = t['cat']\n",
    "        if 'dynamic_feat' in t:\n",
    "            datai[FieldName.FEAT_DYNAMIC_REAL] = t['dynamic_feat']\n",
    "        data.append(datai)\n",
    "    return data\n",
    "def parse_data_ar(dataset):\n",
    "    data = []\n",
    "    for t in dataset:\n",
    "        datai = {FieldName.TARGET: [t['target'][3][:-12],t['target'][8][:-12]], FieldName.START: t['start']}\n",
    "        if 'id' in t:\n",
    "            datai[FieldName.ITEM_ID] = t['id']\n",
    "        if 'cat' in t:\n",
    "            datai[FieldName.FEAT_STATIC_CAT] = t['cat']\n",
    "        if 'dynamic_feat' in t:\n",
    "            datai[FieldName.FEAT_DYNAMIC_REAL] = t['dynamic_feat']\n",
    "        data.append(datai)\n",
    "    return data\n",
    "def parse_data_ar_roll(dataset,start):\n",
    "    data = []\n",
    "    begin=int((start-pd.Timestamp(\"2020-01-02 00:00:00\"))/pd.Timedelta(\"30T\"))\n",
    "    for t in dataset:\n",
    "        datai = {FieldName.TARGET: [t['target'][3][begin:begin+30*48+1],t['target'][8][begin:begin+30*48+1]], FieldName.START: str(start)}\n",
    "        if 'id' in t:\n",
    "            datai[FieldName.ITEM_ID] = t['id']\n",
    "        if 'cat' in t:\n",
    "            datai[FieldName.FEAT_STATIC_CAT] = t['cat']\n",
    "        if 'dynamic_feat' in t:\n",
    "            datai[FieldName.FEAT_DYNAMIC_REAL] = t['dynamic_feat']\n",
    "        data.append(datai)\n",
    "    return data\n",
    "def parse_data_scaling(dataset):# 把所有的销售量进行归一化\n",
    "    data = []\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    for t in dataset:\n",
    "        tar=np.array(t['target'])\n",
    "        tar[1] = scaler.fit_transform(tar[1,:].reshape(-1,1))[:,0]\n",
    "        tar[2] = scaler.fit_transform(tar[2,:].reshape(-1,1))[:,0]\n",
    "        tar[13:] = scaler.fit_transform(tar[13:,:])\n",
    "        datai = {FieldName.TARGET: tar[:,:-12], FieldName.START: t['start']}# 这里放弃最后12个点，即实际训练范围到7.31日下午3点\n",
    "        if 'id' in t:\n",
    "            datai[FieldName.ITEM_ID] = t['id']\n",
    "        if 'cat' in t:\n",
    "            datai[FieldName.FEAT_STATIC_CAT] = t['cat']\n",
    "        if 'dynamic_feat' in t:\n",
    "            datai[FieldName.FEAT_DYNAMIC_REAL] = t['dynamic_feat']\n",
    "        data.append(datai)\n",
    "    return data\n",
    "def parse_data_scaling2(dataset):# 把除了买一价卖一价之外的进行归一化\n",
    "    data = []\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    for t in dataset:\n",
    "        tar=np.array(t['target'])\n",
    "        tar[0]=scaler.fit_transform(tar[0,:].reshape(-1,1))[:,0]\n",
    "        tar[1] = scaler.fit_transform(tar[1,:].reshape(-1,1))[:,0]\n",
    "        tar[2] = scaler.fit_transform(tar[2,:].reshape(-1,1))[:,0]\n",
    "        tar[4:8] = scaler.fit_transform(tar[4:8,:])\n",
    "        tar[9:13] = scaler.fit_transform(tar[9:13,:])\n",
    "        tar[13:] = scaler.fit_transform(tar[13:,:])\n",
    "        datai = {FieldName.TARGET: tar[:,:-12], FieldName.START: t['start']}# 这里放弃最后12个点，即实际训练范围到7.31日下午3点\n",
    "        if 'id' in t:\n",
    "            datai[FieldName.ITEM_ID] = t['id']\n",
    "        if 'cat' in t:\n",
    "            datai[FieldName.FEAT_STATIC_CAT] = t['cat']\n",
    "        if 'dynamic_feat' in t:\n",
    "            datai[FieldName.FEAT_DYNAMIC_REAL] = t['dynamic_feat']\n",
    "        data.append(datai)\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "num_timeseries: 245\n",
      "finish\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.DEBUG)\n",
    "train = load_json('./glts_train_multi_tar.json')\n",
    "test = load_json('./glts_test_multi_tar.json')\n",
    "freq = '30T'\n",
    "import mxnet as mx\n",
    "prediction_length = 1\n",
    "context_length = 48*30\n",
    "num_timeseries = len(train)\n",
    "print('num_timeseries:', num_timeseries)\n",
    "train_ds = ListDataset(parse_data_ar(train), freq=freq, one_dim_target=False)\n",
    "test_ds = ListDataset(parse_data_ar(test), freq=freq, one_dim_target=False)\n",
    "# train_ds = ListDataset(parse_data_scaling(train), freq=freq, one_dim_target=False)\n",
    "# test_ds = ListDataset(parse_data_scaling(test), freq=freq, one_dim_target=False)\n",
    "print('finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "p=os.path.join('./models', 'GPVAR_lr0.001_epoch15scaling_all_ar')\n",
    "from gluonts.model.predictor import Predictor\n",
    "predictor_deserialized = Predictor.deserialize(Path(p))\n",
    "forecast_it, ts_it = make_evaluation_predictions(\n",
    "    dataset=test_ds,  # test dataset\n",
    "    predictor=predictor_deserialized,  # predictor\n",
    "    num_samples=100,  # number of sample paths we want for evaluation\n",
    ")\n",
    "forecasts = list(forecast_it)\n",
    "tss = list(ts_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "samples_shape:  (100, 1, 2)\n",
      "prediction_length:  1\n",
      "Number of sample paths: 100\n",
      "Dimension of samples: (100, 1, 2)\n",
      "Start date of the forecast window: 2020-07-31 18:30:00\n",
      "Frequency of the time series: 30T\n",
      "Mean of the future window:\n",
      " [[9.615109 9.635728]]\n",
      "0.5-quantile (median) of the future window:\n",
      " [[9.628544 9.633638]]\n",
      "(1, 2)\n",
      "0    9.60\n",
      "1    9.61\n",
      "Name: 2020-07-31 18:30:00, dtype: float32\n",
      "2020-07-31 18:30:00\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print('samples_shape: ', forecasts[0].samples.shape)\n",
    "print('prediction_length: ', forecasts[0].prediction_length)\n",
    "forecast_entry=forecasts[18]\n",
    "ts_entry = tss[18]\n",
    "print(f\"Number of sample paths: {forecast_entry.num_samples}\")\n",
    "print(f\"Dimension of samples: {forecast_entry.samples.shape}\")\n",
    "print(f\"Start date of the forecast window: {forecast_entry.start_date}\")\n",
    "print(f\"Frequency of the time series: {forecast_entry.freq}\")\n",
    "print(f\"Mean of the future window:\\n {forecast_entry.mean}\")\n",
    "# print(f\"0.5-quantile (median) of the future window:\\n {forecast_entry.quantile(0.66)[0][[3,8]].shape}\")\n",
    "print(f\"0.5-quantile (median) of the future window:\\n {forecast_entry.quantile(0.52)}\")\n",
    "print(forecast_entry.quantile(0.9).shape)\n",
    "print(ts_entry.iloc[-1])\n",
    "print(ts_entry.index[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def cal_metrics(gt, pre , q):\n",
    "    length = len(forecasts)\n",
    "    mse=0\n",
    "    rmse=0\n",
    "    mae=0\n",
    "    m_ape=0\n",
    "    s_mape=0\n",
    "    for i in range(length):\n",
    "        ts = gt[i].to_numpy()\n",
    "        p = pre[i].quantile(q)[0][np.array([3,8])]\n",
    "        g = ts[-1][np.array([3,8])]\n",
    "        mse += (1/length)*metrics.mean_squared_error(g, p)\n",
    "        rmse += (1/length)*np.sqrt(mse)\n",
    "        mae += (1/length)*metrics.mean_absolute_error(g, p)\n",
    "        m_ape += (1/length)*mape(g, p)\n",
    "        s_mape += (1/length)*smape(g, p)\n",
    "    return mse,rmse,mae,m_ape,s_mape\n",
    "# mse,rmse,mae,m_ape,s_mape=cal_metrics(tss,forecasts,0.55)\n",
    "# print(mse,rmse,mae,m_ape,s_mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "0.46877569057897905 0.30823287156019397 0.14813023213468196 inf 2.9273733217744358\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "/Users/lvhaoran/opt/anaconda3/envs/mxnet/lib/python3.6/site-packages/ipykernel_launcher.py:52: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/Users/lvhaoran/opt/anaconda3/envs/mxnet/lib/python3.6/site-packages/ipykernel_launcher.py:52: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/Users/lvhaoran/opt/anaconda3/envs/mxnet/lib/python3.6/site-packages/ipykernel_launcher.py:52: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/Users/lvhaoran/opt/anaconda3/envs/mxnet/lib/python3.6/site-packages/ipykernel_launcher.py:52: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/Users/lvhaoran/opt/anaconda3/envs/mxnet/lib/python3.6/site-packages/ipykernel_launcher.py:52: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/Users/lvhaoran/opt/anaconda3/envs/mxnet/lib/python3.6/site-packages/ipykernel_launcher.py:52: RuntimeWarning: divide by zero encountered in true_divide\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "def cal_metrics_ar(gt, pre , q):\n",
    "    length = len(forecasts)\n",
    "    mse=0\n",
    "    rmse=0\n",
    "    mae=0\n",
    "    m_ape=0\n",
    "    s_mape=0\n",
    "    for i in range(length):\n",
    "        ts = gt[i].to_numpy()\n",
    "        p = pre[i].mean[0]\n",
    "        g = ts[-1]\n",
    "        mse += (1/length)*metrics.mean_squared_error(g, p)\n",
    "        rmse += (1/length)*np.sqrt(mse)\n",
    "        mae += (1/length)*metrics.mean_absolute_error(g, p)\n",
    "        m_ape += (1/length)*mape(g, p)\n",
    "        s_mape += (1/length)*smape(g, p)\n",
    "    return mse,rmse,mae,m_ape,s_mape\n",
    "mse,rmse,mae,m_ape,s_mape=cal_metrics_ar(tss,forecasts,0.52)\n",
    "print(mse,rmse,mae,m_ape,s_mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# pre_data=[]\n",
    "# pre_data=[\n",
    "#     {\n",
    "#         \"start\":str(pd.Timestamp(tss[0].index.tolist()[-1], freq=freq)),\n",
    "#         \"predict0.3\":val.quantile(0.3)[0].tolist(),\n",
    "#         \"predict0.4\":val.quantile(0.4)[0].tolist(),\n",
    "#         \"predict0.5\":val.quantile(0.5)[0].tolist(),\n",
    "#         \"predictmean\":val.mean[0].tolist()\n",
    "#     } for i, val in enumerate(forecasts)\n",
    "# ]\n",
    "# def write_dicts_to_file(path, data):\n",
    "#     with open(path, 'wb') as fp:\n",
    "#         for d in data:\n",
    "#             fp.write(json.dumps(d).encode(\"utf-8\"))\n",
    "#             fp.write(\"\\n\".encode('utf-8'))\n",
    "# write_dicts_to_file('./predictions_gpvar.json', pre_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "total=load_json('./glts_total_multi_tar.json')\n",
    "def rolling_pre(total, start, predictor=predictor_deserialized, predict_len=60):\n",
    "    delta = pd.Timedelta(\"30T\")\n",
    "    start = start\n",
    "    pre_res_mean = np.zeros((245, 2, predict_len))\n",
    "    pre_res_01 = np.zeros((245, 2, predict_len))\n",
    "    pre_res_09 = np.zeros((245, 2, predict_len))\n",
    "    gt=np.zeros((245, 2, predict_len))\n",
    "    for i in range(predict_len):\n",
    "        print(i)\n",
    "        total_data = ListDataset(parse_data_ar_roll(total,start), freq=freq, one_dim_target=False)\n",
    "        forecast_it, ts_it = make_evaluation_predictions(\n",
    "            dataset = total_data,  # test dataset\n",
    "            predictor = predictor,  # predictor\n",
    "            num_samples = 100,  # number of sample paths we want for evaluation\n",
    "        )\n",
    "        forecasts = list(forecast_it)\n",
    "        tss = list(ts_it)\n",
    "        for j in range(245):\n",
    "            pre_res_mean[j,:,i] = forecasts[j].mean[0]\n",
    "            pre_res_01[j,:,i] = forecasts[j].quantile(0.1)[0]\n",
    "            pre_res_09[j,:,i] = forecasts[j].quantile(0.9)[0]\n",
    "            gt[j,:,i] = tss[j].to_numpy()[-1]\n",
    "        start+=delta\n",
    "    return pre_res_mean, pre_res_01, pre_res_09, gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "181 days 00:00:00\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "start=pd.Timestamp(\"2020-07-01 17:00:00\")\n",
    "print(start-pd.Timestamp(\"2020-01-02 17:00:00\"))\n",
    "# ae=pd.Timestamp(\"2020-01-02 00:00:00\")\n",
    "# print(start-pd.Timedelta(\"30 days\"))\n",
    "# print((start-ae)/pd.Timedelta(\"30T\"))\n",
    "pre_res_mean, pre_res_01, pre_res_09, gt=rolling_pre(total, start)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "finish\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "np.save('./gpvar_mean.npy', pre_res_mean)\n",
    "np.save('./gpvar_01.npy', pre_res_01)\n",
    "np.save('./gpvar_09.npy', pre_res_09)\n",
    "np.save('./gpvar_gt.npy', gt)\n",
    "print('finish')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[[[ 3.61996918e+01  3.61951370e+01  3.61595268e+01 ... -6.99125230e-02\n",
      "    8.07816759e-02 -6.28615022e-02]\n",
      "  [ 3.63182220e+01  3.63438759e+01  3.62102776e+01 ... -1.68078154e-01\n",
      "   -1.05007738e-01  1.30799517e-01]]\n",
      "\n",
      " [[ 7.30395889e+00  7.31332970e+00  7.35648870e+00 ...  5.95084066e-03\n",
      "    1.66509561e-02 -1.19192638e-02]\n",
      "  [ 7.33720827e+00  7.32828712e+00  7.31747818e+00 ...  1.58320661e-04\n",
      "    1.32772028e-02  9.25951637e-03]]\n",
      "\n",
      " [[ 1.93499470e+01  1.94284248e+01  1.93419132e+01 ... -3.02987751e-02\n",
      "   -4.34201136e-02 -3.75012197e-02]\n",
      "  [ 1.93230343e+01  1.94272289e+01  1.93669071e+01 ...  3.09739038e-02\n",
      "   -2.22518556e-02 -3.04613430e-02]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 1.98518217e+00  1.98250127e+00  1.97858214e+00 ... -2.53176130e-03\n",
      "    6.47398038e-03 -7.26233190e-03]\n",
      "  [ 1.99861920e+00  1.98867702e+00  1.99550426e+00 ... -7.52283633e-03\n",
      "   -7.03763403e-03 -1.58877042e-03]]\n",
      "\n",
      " [[ 7.21862221e+00  7.28073978e+00  7.28304482e+00 ...  2.53308434e-02\n",
      "   -8.89940932e-03 -1.70947965e-02]\n",
      "  [ 7.30912209e+00  7.32034683e+00  7.30880594e+00 ... -9.64190811e-03\n",
      "   -1.14667825e-02 -1.35126058e-02]]\n",
      "\n",
      " [[ 7.14263773e+00  7.15253496e+00  7.14753008e+00 ... -1.49721364e-02\n",
      "   -2.97040935e-03 -8.21656897e-04]\n",
      "  [ 7.16165018e+00  7.14905214e+00  7.19305897e+00 ... -7.94965494e-03\n",
      "    2.09023207e-02 -1.12056145e-02]]]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "a=np.load('./gpvar.npy')\n",
    "print(a)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}